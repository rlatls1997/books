# 3장. 카프카 디자인
카프카에선 대용량 실시간 데이터 처리를 위해 배치 전송, 파티션, 분산 기능을 구현함
또한 서버 장애가 발생하더라도 서비스에 영향이 없도록 안정적인 저장일 위해 리플리케이션기능을 구현하고 분산된 서버에서 자동으로 파티션의 리더를 선출하는 기능을 구현함.

## 3.1 카프카 디자인의 특징
카프카에는 높은 처리량과 빠른 메시지 전송, 운영 효율화 등을 위해 분산 시스템, 페이지 캐시, 배치 전송 처리 등의 기능이 구현되었음.

### 1. 분산 시스템
**분산 시스템의 장점**
- 단일 시스템보다 높은 성능
- 시스템 확장에 용이
- 분산 시스템 내 일부 서버의 장애를 극복가능

카프카도 분산시스템이기에 동적으로 서버를 늘릴 수 있다.
리소스 사용량을 고려하여 적절한 수로 유지하면 된다.

### 2. 페이지 캐시
OS는 잔여메모리 일부를 페이지 캐시로 유지하여 성능을 높인다.(메모리에 접근하여 디스크에 접근하지 않아서 빠르다.)
카프카도 OS의 페이지 캐시를 이용하도록 디자인 되었다.

### 3. 배치 전송 처리
작은 I/O가 빈번하게 일어나면 속도저하가 발생할 수 있다.
카프카에서는 작은 I/O를 묶어서 처리할 수 있도록 배치 처리 기능을 제공한다.


## 3.1 카프카 데이터 모델
- 토픽 : 메시지를 받을 수 있도록 논리적으로 묶은 개념
- 파티션 : 토픽을 구성하는 데이터 저장소. 수평확장이 가능한 단위

### 1. 토픽의 이해
카프카 클러스터는 토픽이라는 곳에 데이터를 저장한다.
카프카에서는 데이터를 구분하기 위한 단위로 토픽이라는 용어를 사용한다.

토픽 네이밍 컨벤션은 따로 없으나 토픽 이름에 접두어로 서비스명을 추가하는 것이 좋다.(토픽 이름 중복 제거에 좋음)
ex) sbs-news, sbs-video, kbs-news, kbs-video

### 2. 파티션의 이해
파티션은 토픽을 분할한 것이다.

메시징 큐 시스템은 순서를 보장해야한다는 제약때문에 이전 메시지가 처리되어야 다음 메시지를 처리하게 된다.
카프카에서 효율적인 메시지 전송 속도를 높이려면 토픽의 파티션 수를 늘려줘야한다.

**파티션을 무작정 늘렸을 때의 영향**
- 파일 핸들러 낭비 : 각 파티션은 브로커의 디렉토리와 매핑되고 데이터마다 2개의 파일이 있다.(인덱스, 실제데이터)
카프카에선 모든 디렉토리의 파일들에 대해 파일 핸들을 열게 되므로 파티션 수가 많을수록 파일 핸들 수 또한 많아져서 리소스를 낭비하게 된다.
- 장애 복구 시간 증가 : 카프카는 높은 가용성을 위해 리플리케이션을 지원한다. 만약 파티션이 여러개인데 메시지 브로커가 갑자기 종료되면 각 파티션의 리더를 다른 브로커가 있는 곳으로 이동시켜야한다. 이 때 파티션 수가 많아서 해당 작업을 함느라 장애시간이 길어질 수 있다.

**토픽의 적절한 파티션 수는?**
원하는 목표 처리량을 기준으로 잡아야 한다.

카프카에서 파티션 수를 줄이는 기능은 제공되지 않는다.
적절한 파티션 수를 측정하기 어렵다면 조금씩 파티션 수를 늘려나가자.

카프카에서는 브로커당 최대 파티션 수를 약 2,000개 정도로 권장한다.

### 3. 오프셋과 메시지 순서
카프카에서는 각 파티션마다 메시지가 저장되는 위치를 오프셋이라고 부른다.
오프셋은 파티션 내에서만 유일하다.
오프셋은 순차적으로 증가하는 64비트 정수의 형태이다.

카프카에서는 오프셋을 이용하여 메시지의 순서를 보장한다.
오프셋 순서가 바뀐 상태로는 절대 메시지를 가져갈 수 없다.


## 3.3 카프카의 고가용성과 리플리케이션
카프카는 분산 애플리케이션으로 서버의 물리적 장애가 발생하는 경우에도 높은 고가용성을 보장한다.
카프카는 리플리케이션 기능을 제공한다.
카프카의 리플리케이션은 토픽을 이루는 각각의 파티션을 리플리케이션한다.

### 1. 리플리케이션 팩터와 리더, 팔로워의 역할
- 리플리케이션 팩터 설정
```shell
vi /usr/local/kafka/config/server.properties

#default.replication.factor 항목을 원하는 숫자로 변경한다.
# 이 설정값은 아무런 옵션을 주지 않고 토픽을 생성할 때 적용된다.
# 옵션을 주면 각 토픽별로 다른 리플리케이션 팩터값을 설정할 수 있다.
# 운영중에도 값을 변경할 수 있다.
```

리플리케이션 팩터는 클러스터 내 모든 브로커에 동일하게 설정해야한다.
설정 내용 변경 후 브로커를 1대씩 재시작하면 변경 내용이 적용된다.

리플리케이션을 통해 여러 브로커에 토픽 파티션을 복제할 수 있다.
카프카에서는 
원본은 리더,
복제본은 팔로워라고 부른다.

리더가 있는 브로커가 다운되더라도 팔로워가 새 리더가 되어 요청에 응답하게 되어 장애를 극복할 수 있다.

**리플리케이션의 단점**
- 리플리케이션 팩터만큼 각 브로커에 복제파티션을 저장할 저장소가 필요하다.
- 브로커의 리소스 사용량이 증가한다.(리플리케이션을 위한 작업 등)

### 2. 리더와 팔로워의 관리
카프카에서의 리플리케이션 작업은..
리더는 모든 데이터의 읽기 쓰기에 대한 요청에 응답하면서 데이터를 저장해나간다.
팔로워는 리더를 주기적으로 보면서 자신에게 없는 데이터를 리더로부터 주기적으로 가져오는 방법으로 리플리케이션을 유지한다.

만약 팔로워에 문제가 있어서 데이터를 잘 가져오지 못하는 경우, 기존 리더가 다운되어 팔로워가 새 리더로 승격되었을 때 데이터 불일치 문제가 발생할 수 있다.
</br>카프카에서는 이런 현상을 방지하기 위해 ISR(In Sync Replica)라는 개념을 도입했다.

ISR : 현재 리플리케이션되고 있는 리플리케이션 그룹
ISR에는 ISR에 속한 구성원만이 리더의 자격을 가질 수 있다는 규칙이 있다.
팔로워는 항상 리더가 될 준비가 되어있어야 하기 때문에 데이터 동기화 작업을 매우 중요하게 처리한다.


**리플리케이션 팩터가 3인 토픽의 ISR의 동작과정**
1. 프로듀서가 메시지를 리더에게 보낸다. 
2. 메시지는 토픽의 리더만이 읽고 쓸 수 있으므로 요청을 받고 저장한다.
팔로워들은 매우 짦은 주기로 리더에게 새로 저장된 메시지가 잇는지 확인한다.
</br>팔로워 1은 잘 동작하지만 팔로워2가 잘 동작하지 않는다.
</br>리더는 팔로워들이 주기적으로 데이터를 확인하고 있는지 확인한다.
</br>만약 설정된 주기(replica.lag.time.max.ms)만큼 확인 요청이 오지 않는다면 리더의 역할을 대신할 수 없다고 판단하여 ISR에서 해당 팔로워를 추방한다.
3. ISR그룹 구성원이 3에서 2로 감소한다.

## 3.4 모든 브로커가 다운된다면
**시나리오**
- 카프카 클러스터를 3대의 브로커로 구성
- 리플리케이션 팩터는 3
- 프로듀서가 데이터를 보내는 동안 브로커가 1대씩 다운
- 최종적으로 모든 브로커 다운

**다운된 상황에서의 ISR**
- 팔로워2가 다운되어 ISR에서 제외된다.
  </br>매시지는 리더와 팔로워1에만 저장된다.
- 팔로워1가 다운되어 ISR에서 제외된다.
  </br>매시지는 리더에만 저장된다.
- 리더가 있는 브로커까지 다운된다.
</br>리더가 존재하지 않기 때문에 메시지를 보낼 수 없다.

**위 상황에서 가능한 문제 해결 방법** 
1. 마지막에 다운되었던 리더가 살아나길 바란다.
<br>마지막 리더에는 프로듀서가 보낸 메시지가 모두 저장되어 있기 때문에 살아나기만 한다면 메시지 손실 없이 서비스를 지속적으로 제공할 수 있다.
<br>여기서 필수조건은 `마지막 리더가 반드시 시작되어야 한다.(또 가장 먼저 시작되어야 한다.)`
<br>(생각해보니까 가장 먼저 살아나지 않아도 되는 이유가 어차피 메시지는 저장되어 있는 상태고 파티션마다 독립적이니까 살아나기만 한다면 메시지 손실은 걱정 안해도 될듯)
<br>(나는 가장 먼저 살아나야 되는 이유가 마지막에 다운된 브로커가 리더로 선출되어야 다른 브로커에도 마지막에 들어왔던 메시지가 복제된다고 생각했는데 메시지가 잘 처리만 된다면 굳이 그럴 필요가 없을듯 하다.)
<br>(근데 2번 케이스에서는 메시지 손실이 발생된다고 하네?)

2. ISR에서 추방되었지만 먼저 살아나면 자동으로 리더가 된다.
마지막에 죽은 리더가 아닌 ISR에서 추방되었던 팔로워가 먼자 살아나서 리더가 된다면 메시지 손실이 발생할 수 있다.
<br>
카프카에서는 일부 메시지가 유실되어도 빠른 정상화를 위해 2번 방식을 추천한다.

1번 또는 2번 중 어떤방식으로 리더를 재선출할지는 옵션으로 변경할 수 있다.
```shell
vi /usr/local/kafka/config/server.properties

# unclean.leader.election.enable=true
# false : 메시지 손실 관점에서 마지막 리더를 기다린다.(1번방법)
# true : 서비스적 관점에서 메시지 손실을 감수하고 빠르게 서비스를 정상화한다.(2번방법)
```

## 3.5 카프카에서 사용하는 주키퍼 지노드 역할
카프카에서 사용하는 지노드를 알아보자
- controller : 현재 카프카 클러스터의 컨트롤러 정보를 확인할 수 있다. 
<br>카프카는 클러스터 내 브로커 하나를 컨트롤러로 선정하는데 컨트롤러는 브로커 레벨에서 실패를 감지하고 브로커에 영향을 받는 모든 파티션의 리더 변경을 책임진다.
- brokers : 브로커 관련된 정보들이 저장되며 카프카를 설치할 때 브로커 config에서 수정한 broker.id를 확인할 수 있다.
<br>브로커는 시작시 /brokes/ids에 broker.id로 지노드를 작성하여 자신을 등록한다.
<br>/borkes/topics라는 지노드에서는 토픽의 파티션 수 , ISR 구성정보, 리더 종보등 클러스터 내 토픽 정보들을 확인할 수 있다.
- consumer : 컨수머 관련된 정보들이 저장된다. 또한 오프셋 정보가 이곳에 저장되며 오프셋 정보는 지워지면 안되기 때문에 주키퍼의 영구노드로 저장된다.
- config : 토픽의 상세 설정 정보를 확인할 수 있다. 토픽 생성 시 기본으로 설정되는 옵션 외에 상세 설정 추가를 통해 설정된 별도의 옵션 정보도 여기서 확인할 수 있다.

**임시노드와 영구노드**
주키퍼의 지노드는 영구노드와 임시노드로 나뉘어진다.
- 영구노드 : 영구노드는 delete를 호출하여 삭제할 수 있다.
- 임시노드 : 생성한 클라이언트의 연결이 끊어지거나 장애가 발생하면 삭제된다.

## 생각해보기
- ISR의 브로커가 전부 다운되었을 때 왜 마지막에 다운되었던 리더가 가장 먼저 살아나지 않고 다른 브로커가 먼저 살아나면 메시지 솔싫이 발생하는지 이해가 되지 않는다.
<br>마지막에 죽었던 리더에는 어쨋든 토픽 파티션에 메시지가 저장되어 있으니 메시지 손실은 걱정없는 것 아닌가?
<br>=> 마지막에 죽었던 리더가 새로운 리더로부터 동기화 작업을 거치면서 죽기 전에 저장되었던 메시지가 유실된다!!
- 임시노드, 영구노드는 무엇인가



